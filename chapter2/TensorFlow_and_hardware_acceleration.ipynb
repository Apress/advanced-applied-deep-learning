{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TensorFlow and hardware acceleration",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michelucci/aadl2-code/blob/master/chapter2/TensorFlow_and_hardware_acceleration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "eFu2x_NvJQV0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# TensorFlow and hardware acceleration\n",
        "\n",
        "(C) 2019 Umberto Michelucci"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "BlmQIFSLZDdc"
      },
      "cell_type": "markdown",
      "source": [
        "# Confirm TensorFlow can see the GPU\n",
        "\n",
        "Simply select \"GPU\" in the Accelerator drop-down in Notebook Settings (either through the Edit menu or the command palette at cmd/ctrl-shift-P)."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "3IEVK-KFxi5Z",
        "outputId": "9e62871b-a744-445f-c668-5b675a676b5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "QXRh0DPiZRyG"
      },
      "cell_type": "markdown",
      "source": [
        "# Observe TensorFlow speedup on GPU relative to CPU\n",
        "\n",
        "This example constructs a matrix multiplication between two big tensors to check the performance improvements."
      ]
    },
    {
      "metadata": {
        "id": "W0Xj3Y92lN8j",
        "colab_type": "code",
        "outputId": "1817af31-f46e-441f-d6c7-c156d88ff658",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "print(tf.test.is_gpu_available())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QKrjRr8kH8Yk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The following function will return in a list, all the devices available. If you have enabled GPU acceleration for this notebook in google Colab for example you should see two GPUs devices."
      ]
    },
    {
      "metadata": {
        "id": "VFLLUd79nfUN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "\n",
        "def get_available_gpus():\n",
        "    local_device_protos = device_lib.list_local_devices()\n",
        "    return [x.name for x in local_device_protos if x.device_type.endswith('GPU')]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "565DHGnZpxAs",
        "colab_type": "code",
        "outputId": "fe1de030-319c-4874-aab8-7bb9b253698e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "get_available_gpus()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/device:XLA_GPU:0', '/device:GPU:0']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "QCU76cUloB_m",
        "colab_type": "code",
        "outputId": "4a27478a-a256-4fe9-ccfa-e8a7688fbba0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        }
      },
      "cell_type": "code",
      "source": [
        "  local_device_protos = device_lib.list_local_devices()\n",
        "  print(local_device_protos)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 3004393512909135219\n",
            ", name: \"/device:XLA_CPU:0\"\n",
            "device_type: \"XLA_CPU\"\n",
            "memory_limit: 17179869184\n",
            "locality {\n",
            "}\n",
            "incarnation: 16323739484962476874\n",
            "physical_device_desc: \"device: XLA_CPU device\"\n",
            ", name: \"/device:XLA_GPU:0\"\n",
            "device_type: \"XLA_GPU\"\n",
            "memory_limit: 17179869184\n",
            "locality {\n",
            "}\n",
            "incarnation: 16797530695469281809\n",
            "physical_device_desc: \"device: XLA_GPU device\"\n",
            ", name: \"/device:GPU:0\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 11281553818\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 904680829004167430\n",
            "physical_device_desc: \"device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7\"\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "y2SKJSINo8nK",
        "colab_type": "code",
        "outputId": "d7cff4ac-23dd-4831-d001-2ec48097727f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "type(local_device_protos)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "B7hr_wdVoDS_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Performance comparison"
      ]
    },
    {
      "metadata": {
        "id": "-nAo5NGUW8Nu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import timeit\n",
        "\n",
        "# See https://www.tensorflow.org/tutorials/using_gpu#allowing_gpu_memory_growth\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UV34gWhrXNFl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sess = tf.Session(config=config)\n",
        "#sess.run(tf.global_variables_initializer())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "72NbmQsxXBRf",
        "colab_type": "code",
        "outputId": "7c216193-c464-4024-938d-e7cb72934116",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "with tf.device('/gpu:0'):\n",
        "  tensor1 = tf.random_normal((10000, 10000))\n",
        "  tensor2 = tf.random_normal((10000, 10000))\n",
        "  prod = tf.linalg.matmul(tensor1, tensor2)\n",
        "  prod_sum = tf.reduce_sum(prod)\n",
        "  \n",
        "  sess.run(prod_sum)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 765 ms, sys: 541 ms, total: 1.31 s\n",
            "Wall time: 1.56 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sG4R8gbFW9P_",
        "colab_type": "code",
        "outputId": "4be2c63b-6b40-44e1-aac8-3d15cb7c3ca8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "%%time\n",
        "with tf.device('/cpu:0'):\n",
        "  tensor1 = tf.random_normal((10000, 10000))\n",
        "  tensor2 = tf.random_normal((10000, 10000))\n",
        "  prod = tf.linalg.matmul(tensor1, tensor2)\n",
        "  prod_sum = tf.reduce_sum(prod)\n",
        "  \n",
        "  sess.run(prod_sum)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1min 5s, sys: 822 ms, total: 1min 6s\n",
            "Wall time: 33.8 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "H-HhAox9al9U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Smaller matrices\n",
        "\n",
        "Now let's try with smaller matrixes ```100x100```"
      ]
    },
    {
      "metadata": {
        "id": "AibrKx2SafPb",
        "colab_type": "code",
        "outputId": "8258ba1a-a8ce-4c35-b6da-7a1a014a4a69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "with tf.device('/gpu:0'):\n",
        "  tensor1 = tf.random_normal((100, 100))\n",
        "  tensor2 = tf.random_normal((100, 100))\n",
        "  prod = tf.linalg.matmul(tensor1, tensor2)\n",
        "  prod_sum = tf.reduce_sum(prod)\n",
        "  \n",
        "  sess.run(prod_sum)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 21.3 ms, sys: 1.05 ms, total: 22.3 ms\n",
            "Wall time: 24.7 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aMjwbV1yahld",
        "colab_type": "code",
        "outputId": "4f6599ae-c089-4404-8fc6-9c61563a336a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "with tf.device('/cpu:0'):\n",
        "  tensor1 = tf.random_normal((100, 100))\n",
        "  tensor2 = tf.random_normal((100, 100))\n",
        "  prod = tf.linalg.matmul(tensor1, tensor2)\n",
        "  prod_sum = tf.reduce_sum(prod)\n",
        "  \n",
        "  sess.run(prod_sum)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 18.3 ms, sys: 2.01 ms, total: 20.3 ms\n",
            "Wall time: 19.9 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3hnS6sJrakhG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now the difference is negligible."
      ]
    }
  ]
}